{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-11-05T19:27:17.374500Z","iopub.status.busy":"2021-11-05T19:27:17.374105Z","iopub.status.idle":"2021-11-05T19:27:17.480348Z","shell.execute_reply":"2021-11-05T19:27:17.479348Z","shell.execute_reply.started":"2021-11-05T19:27:17.374465Z"},"trusted":true},"outputs":[],"source":["import optuna\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from lightgbm import LGBMClassifier\n","from optuna.samplers import TPESampler\n","from sklearn.metrics import accuracy_score, roc_auc_score\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import StratifiedKFold\n","from xgboost import XGBClassifier"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2021-11-05T19:19:38.797040Z","iopub.status.busy":"2021-11-05T19:19:38.796748Z","iopub.status.idle":"2021-11-05T19:20:16.504560Z","shell.execute_reply":"2021-11-05T19:20:16.503263Z","shell.execute_reply.started":"2021-11-05T19:19:38.797006Z"},"trusted":true},"outputs":[],"source":["train = pd.read_csv(\"train.csv\", index_col='id')\n","test = pd.read_csv(\"test.csv\", index_col='id')\n","\n","sample_submission = pd.read_csv('sample_solution.csv')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2021-11-05T19:20:16.506910Z","iopub.status.busy":"2021-11-05T19:20:16.506633Z","iopub.status.idle":"2021-11-05T19:20:16.542664Z","shell.execute_reply":"2021-11-05T19:20:16.541813Z","shell.execute_reply.started":"2021-11-05T19:20:16.506877Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>f1</th>\n","      <th>f2</th>\n","      <th>f3</th>\n","      <th>f4</th>\n","      <th>f5</th>\n","      <th>f6</th>\n","      <th>f7</th>\n","      <th>f8</th>\n","      <th>f9</th>\n","      <th>f10</th>\n","      <th>...</th>\n","      <th>f110</th>\n","      <th>f111</th>\n","      <th>f112</th>\n","      <th>f113</th>\n","      <th>f114</th>\n","      <th>f115</th>\n","      <th>f116</th>\n","      <th>f117</th>\n","      <th>f118</th>\n","      <th>claim</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.10859</td>\n","      <td>0.004314</td>\n","      <td>-37.566</td>\n","      <td>0.017364</td>\n","      <td>0.28915</td>\n","      <td>-10.25100</td>\n","      <td>135.12</td>\n","      <td>168900.0</td>\n","      <td>3.992400e+14</td>\n","      <td>86.489</td>\n","      <td>...</td>\n","      <td>-12.2280</td>\n","      <td>1.7482</td>\n","      <td>1.90960</td>\n","      <td>-7.11570</td>\n","      <td>4378.80</td>\n","      <td>1.2096</td>\n","      <td>8.613400e+14</td>\n","      <td>140.1</td>\n","      <td>1.01770</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.10090</td>\n","      <td>0.299610</td>\n","      <td>11822.000</td>\n","      <td>0.276500</td>\n","      <td>0.45970</td>\n","      <td>-0.83733</td>\n","      <td>1721.90</td>\n","      <td>119810.0</td>\n","      <td>3.874100e+15</td>\n","      <td>9953.600</td>\n","      <td>...</td>\n","      <td>-56.7580</td>\n","      <td>4.1684</td>\n","      <td>0.34808</td>\n","      <td>4.14200</td>\n","      <td>913.23</td>\n","      <td>1.2464</td>\n","      <td>7.575100e+15</td>\n","      <td>1861.0</td>\n","      <td>0.28359</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.17803</td>\n","      <td>-0.006980</td>\n","      <td>907.270</td>\n","      <td>0.272140</td>\n","      <td>0.45948</td>\n","      <td>0.17327</td>\n","      <td>2298.00</td>\n","      <td>360650.0</td>\n","      <td>1.224500e+13</td>\n","      <td>15827.000</td>\n","      <td>...</td>\n","      <td>-5.7688</td>\n","      <td>1.2042</td>\n","      <td>0.26290</td>\n","      <td>8.13120</td>\n","      <td>45119.00</td>\n","      <td>1.1764</td>\n","      <td>3.218100e+14</td>\n","      <td>3838.2</td>\n","      <td>0.40690</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.15236</td>\n","      <td>0.007259</td>\n","      <td>780.100</td>\n","      <td>0.025179</td>\n","      <td>0.51947</td>\n","      <td>7.49140</td>\n","      <td>112.51</td>\n","      <td>259490.0</td>\n","      <td>7.781400e+13</td>\n","      <td>-36.837</td>\n","      <td>...</td>\n","      <td>-34.8580</td>\n","      <td>2.0694</td>\n","      <td>0.79631</td>\n","      <td>-16.33600</td>\n","      <td>4952.40</td>\n","      <td>1.1784</td>\n","      <td>4.533000e+12</td>\n","      <td>4889.1</td>\n","      <td>0.51486</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.11623</td>\n","      <td>0.502900</td>\n","      <td>-109.150</td>\n","      <td>0.297910</td>\n","      <td>0.34490</td>\n","      <td>-0.40932</td>\n","      <td>2538.90</td>\n","      <td>65332.0</td>\n","      <td>1.907200e+15</td>\n","      <td>144.120</td>\n","      <td>...</td>\n","      <td>-13.6410</td>\n","      <td>1.5298</td>\n","      <td>1.14640</td>\n","      <td>-0.43124</td>\n","      <td>3856.50</td>\n","      <td>1.4830</td>\n","      <td>-8.991300e+12</td>\n","      <td>NaN</td>\n","      <td>0.23049</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 119 columns</p>\n","</div>"],"text/plain":["         f1        f2         f3        f4       f5        f6       f7  \\\n","id                                                                       \n","0   0.10859  0.004314    -37.566  0.017364  0.28915 -10.25100   135.12   \n","1   0.10090  0.299610  11822.000  0.276500  0.45970  -0.83733  1721.90   \n","2   0.17803 -0.006980    907.270  0.272140  0.45948   0.17327  2298.00   \n","3   0.15236  0.007259    780.100  0.025179  0.51947   7.49140   112.51   \n","4   0.11623  0.502900   -109.150  0.297910  0.34490  -0.40932  2538.90   \n","\n","          f8            f9        f10  ...     f110    f111     f112  \\\n","id                                     ...                             \n","0   168900.0  3.992400e+14     86.489  ... -12.2280  1.7482  1.90960   \n","1   119810.0  3.874100e+15   9953.600  ... -56.7580  4.1684  0.34808   \n","2   360650.0  1.224500e+13  15827.000  ...  -5.7688  1.2042  0.26290   \n","3   259490.0  7.781400e+13    -36.837  ... -34.8580  2.0694  0.79631   \n","4    65332.0  1.907200e+15    144.120  ... -13.6410  1.5298  1.14640   \n","\n","        f113      f114    f115          f116    f117     f118  claim  \n","id                                                                    \n","0   -7.11570   4378.80  1.2096  8.613400e+14   140.1  1.01770      1  \n","1    4.14200    913.23  1.2464  7.575100e+15  1861.0  0.28359      0  \n","2    8.13120  45119.00  1.1764  3.218100e+14  3838.2  0.40690      1  \n","3  -16.33600   4952.40  1.1784  4.533000e+12  4889.1  0.51486      1  \n","4   -0.43124   3856.50  1.4830 -8.991300e+12     NaN  0.23049      1  \n","\n","[5 rows x 119 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["train.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2021-11-05T19:20:16.544810Z","iopub.status.busy":"2021-11-05T19:20:16.544481Z","iopub.status.idle":"2021-11-05T19:20:16.550525Z","shell.execute_reply":"2021-11-05T19:20:16.549622Z","shell.execute_reply.started":"2021-11-05T19:20:16.544767Z"},"trusted":true},"outputs":[],"source":["features = [x for x in train.columns if x.startswith('f')]"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2021-11-05T19:20:16.553903Z","iopub.status.busy":"2021-11-05T19:20:16.553215Z","iopub.status.idle":"2021-11-05T19:20:34.458177Z","shell.execute_reply":"2021-11-05T19:20:34.456943Z","shell.execute_reply.started":"2021-11-05T19:20:16.553861Z"},"trusted":true},"outputs":[],"source":["train['n_missing'] = train[features].isna().sum(axis=1)\n","train['abs_sum'] = train[features].abs().sum(axis=1)\n","train['std'] = train[features].std(axis=1)\n","train['avg'] = train[features].mean(axis=1)\n","train['max'] = train[features].max(axis=1)\n","train['min'] = train[features].min(axis=1)\n","\n","test['n_missing'] = test[features].isna().sum(axis=1)\n","test['abs_sum'] = test[features].abs().sum(axis=1)\n","test['std'] = test[features].std(axis=1)\n","test['avg'] = test[features].mean(axis=1)\n","test['max'] = test[features].min(axis=1)\n","test['min'] = test[features].min(axis=1)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2021-11-05T19:20:34.460380Z","iopub.status.busy":"2021-11-05T19:20:34.460053Z","iopub.status.idle":"2021-11-05T19:20:40.450182Z","shell.execute_reply":"2021-11-05T19:20:40.448724Z","shell.execute_reply.started":"2021-11-05T19:20:34.460339Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Wall time: 3.59 s\n"]}],"source":["%%time\n","train = train.apply(lambda x: x.fillna(x.median()),axis=0)\n","test = test.apply(lambda x: x.fillna(x.median()), axis=0)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2021-11-05T19:20:40.452678Z","iopub.status.busy":"2021-11-05T19:20:40.451871Z","iopub.status.idle":"2021-11-05T19:20:42.697938Z","shell.execute_reply":"2021-11-05T19:20:42.697071Z","shell.execute_reply.started":"2021-11-05T19:20:40.452617Z"},"trusted":true},"outputs":[],"source":["X, X_val, y, y_val = train_test_split(train.drop('claim', axis=1), train.claim, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2021-11-05T19:20:42.699417Z","iopub.status.busy":"2021-11-05T19:20:42.699162Z","iopub.status.idle":"2021-11-05T19:25:47.361204Z","shell.execute_reply":"2021-11-05T19:25:47.360144Z","shell.execute_reply.started":"2021-11-05T19:20:42.699387Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2021-11-06 10:05:16,503]\u001b[0m A new study created in memory with name: no-name-c6a657ce-5b4b-47cd-9655-ed680128fda9\u001b[0m\n","\u001b[32m[I 2021-11-06 10:05:38,202]\u001b[0m Trial 0 finished with value: 0.7719294509456572 and parameters: {'max_depth': 5, 'n_estimators': 93, 'learning_rate': 0.1834348715226848, 'num_leaves': 1097, 'min_child_samples': 191}. Best is trial 0 with value: 0.7719294509456572.\u001b[0m\n","\u001b[32m[I 2021-11-06 10:06:06,916]\u001b[0m Trial 1 finished with value: 0.7694283604195409 and parameters: {'max_depth': 6, 'n_estimators': 103, 'learning_rate': 0.4458328082703159, 'num_leaves': 1240, 'min_child_samples': 77}. Best is trial 0 with value: 0.7719294509456572.\u001b[0m\n","\u001b[32m[I 2021-11-06 10:06:20,740]\u001b[0m Trial 2 finished with value: 0.7718885401519257 and parameters: {'max_depth': 4, 'n_estimators': 88, 'learning_rate': 0.33370867776816077, 'num_leaves': 2921, 'min_child_samples': 154}. Best is trial 0 with value: 0.7719294509456572.\u001b[0m\n","\u001b[32m[I 2021-11-06 10:06:42,586]\u001b[0m Trial 3 finished with value: 0.7713468607720759 and parameters: {'max_depth': 4, 'n_estimators': 150, 'learning_rate': 0.05641167338594236, 'num_leaves': 2393, 'min_child_samples': 160}. Best is trial 0 with value: 0.7719294509456572.\u001b[0m\n","\u001b[32m[I 2021-11-06 10:06:50,227]\u001b[0m Trial 4 finished with value: 0.7713895765507266 and parameters: {'max_depth': 3, 'n_estimators': 21, 'learning_rate': 0.6174815478795656, 'num_leaves': 23, 'min_child_samples': 91}. Best is trial 0 with value: 0.7719294509456572.\u001b[0m\n","\u001b[32m[I 2021-11-06 10:07:01,856]\u001b[0m Trial 5 finished with value: 0.771582364742217 and parameters: {'max_depth': 2, 'n_estimators': 59, 'learning_rate': 0.3998610317291583, 'num_leaves': 2049, 'min_child_samples': 190}. Best is trial 0 with value: 0.7719294509456572.\u001b[0m\n","\u001b[32m[I 2021-11-06 10:07:25,043]\u001b[0m Trial 6 finished with value: 0.7717087114524587 and parameters: {'max_depth': 4, 'n_estimators': 108, 'learning_rate': 0.5142344869901677, 'num_leaves': 2881, 'min_child_samples': 133}. Best is trial 0 with value: 0.7719294509456572.\u001b[0m\n","\u001b[32m[I 2021-11-06 10:07:42,277]\u001b[0m Trial 7 finished with value: 0.7690508884597139 and parameters: {'max_depth': 6, 'n_estimators': 51, 'learning_rate': 0.6803075705570258, 'num_leaves': 2890, 'min_child_samples': 169}. Best is trial 0 with value: 0.7719294509456572.\u001b[0m\n","\u001b[32m[I 2021-11-06 10:08:04,344]\u001b[0m Trial 8 finished with value: 0.7707838923239108 and parameters: {'max_depth': 3, 'n_estimators': 132, 'learning_rate': 0.9422017614646772, 'num_leaves': 2063, 'min_child_samples': 11}. Best is trial 0 with value: 0.7719294509456572.\u001b[0m\n","\u001b[32m[I 2021-11-06 10:08:17,764]\u001b[0m Trial 9 finished with value: 0.7714292353251829 and parameters: {'max_depth': 3, 'n_estimators': 53, 'learning_rate': 0.23089390253276645, 'num_leaves': 2141, 'min_child_samples': 113}. Best is trial 0 with value: 0.7719294509456572.\u001b[0m\n","\u001b[32m[I 2021-11-06 10:08:25,343]\u001b[0m Trial 10 finished with value: 0.5 and parameters: {'max_depth': 5, 'n_estimators': 1, 'learning_rate': 0.0047248278632273655, 'num_leaves': 798, 'min_child_samples': 43}. Best is trial 0 with value: 0.7719294509456572.\u001b[0m\n","\u001b[32m[I 2021-11-06 10:08:47,521]\u001b[0m Trial 11 finished with value: 0.771823221384919 and parameters: {'max_depth': 5, 'n_estimators': 91, 'learning_rate': 0.23129881194038893, 'num_leaves': 696, 'min_child_samples': 200}. Best is trial 0 with value: 0.7719294509456572.\u001b[0m\n","\u001b[32m[I 2021-11-06 10:09:05,431]\u001b[0m Trial 12 finished with value: 0.7718186369684299 and parameters: {'max_depth': 5, 'n_estimators': 77, 'learning_rate': 0.2095753522878902, 'num_leaves': 1392, 'min_child_samples': 149}. Best is trial 0 with value: 0.7719294509456572.\u001b[0m\n","\u001b[32m[I 2021-11-06 10:09:28,436]\u001b[0m Trial 13 finished with value: 0.7719047420779903 and parameters: {'max_depth': 5, 'n_estimators': 118, 'learning_rate': 0.12683395835502798, 'num_leaves': 658, 'min_child_samples': 199}. Best is trial 0 with value: 0.7719294509456572.\u001b[0m\n","\u001b[32m[I 2021-11-06 10:09:58,137]\u001b[0m Trial 14 finished with value: 0.7717204054661496 and parameters: {'max_depth': 5, 'n_estimators': 125, 'learning_rate': 0.0837086524563389, 'num_leaves': 66, 'min_child_samples': 177}. Best is trial 0 with value: 0.7719294509456572.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Optimized on ROC AUC SCORE\n","Optimized LightGBM accuracy:  0.7714788291297812\n","Optimized LightGBM roc-auc-score:  0.7719294509456572\n"]}],"source":["def create_model(trial):\n","    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n","    n_estimators = trial.suggest_int(\"n_estimators\", 1, 150)\n","    learning_rate = trial.suggest_uniform('learning_rate', 0.0000001, 1)\n","    num_leaves = trial.suggest_int(\"num_leaves\", 2, 3000)\n","    min_child_samples = trial.suggest_int('min_child_samples', 3, 200)\n","    model = LGBMClassifier(\n","        learning_rate=learning_rate, \n","        n_estimators=n_estimators, \n","        max_depth=max_depth, \n","        num_leaves=num_leaves, \n","        min_child_samples=min_child_samples,\n","        random_state=42\n","    )\n","    return model\n","\n","class Optimizer:\n","    def __init__(self, metric, trials=15):\n","        self.metric = metric\n","        self.trials = trials\n","        self.sampler = TPESampler(seed=42)\n","        \n","    def objective(self, trial):\n","        model = create_model(trial)\n","        model.fit(X, y)\n","        preds = model.predict(X_val)\n","        if self.metric == 'acc':\n","            return accuracy_score(y_val, preds)\n","        else:\n","            return roc_auc_score(y_val, preds)\n","            \n","    def optimize(self):\n","        study = optuna.create_study(direction=\"maximize\", sampler=self.sampler)\n","        study.optimize(self.objective, n_trials=self.trials)\n","        return study.best_params\n","    \n","optimizer = Optimizer('f1')\n","lgb_f1_params = optimizer.optimize()\n","lgb_f1_params['random_state'] = 42\n","lgb_f1 = LGBMClassifier(\n","    **lgb_f1_params\n",")\n","lgb_f1.fit(X, y)\n","preds = lgb_f1.predict(X_val)\n","\n","print('Optimized on ROC AUC SCORE')\n","print('Optimized LightGBM accuracy: ', accuracy_score(y_val, preds))\n","print('Optimized LightGBM roc-auc-score: ', roc_auc_score(y_val, preds, average='macro'))"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2021-11-05T19:27:22.963124Z","iopub.status.busy":"2021-11-05T19:27:22.962818Z","iopub.status.idle":"2021-11-05T20:45:02.045693Z","shell.execute_reply":"2021-11-05T20:45:02.044655Z","shell.execute_reply.started":"2021-11-05T19:27:22.963092Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2021-11-06 10:10:19,137]\u001b[0m A new study created in memory with name: no-name-b6b04ecd-44db-488d-8eb3-6f83fda35380\u001b[0m\n","C:\\Users\\dmytro.hlushenkov\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[10:10:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2021-11-06 10:17:21,308]\u001b[0m Trial 0 finished with value: 0.7717304860821509 and parameters: {'max_depth': 5, 'n_estimators': 93, 'learning_rate': 0.1834348715226848, 'gamma': 0.7796910223036693, 'subsample': 0.5968904729306923}. Best is trial 0 with value: 0.7717304860821509.\u001b[0m\n","C:\\Users\\dmytro.hlushenkov\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[10:17:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2021-11-06 10:19:46,069]\u001b[0m Trial 1 finished with value: 0.7698574441590116 and parameters: {'max_depth': 3, 'n_estimators': 75, 'learning_rate': 0.459248946040978, 'gamma': 0.33370867776816077, 'subsample': 0.1429525312401486}. Best is trial 0 with value: 0.7717304860821509.\u001b[0m\n","C:\\Users\\dmytro.hlushenkov\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[10:19:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2021-11-06 10:27:25,906]\u001b[0m Trial 2 finished with value: 0.7714557076563047 and parameters: {'max_depth': 4, 'n_estimators': 150, 'learning_rate': 0.05641167338594236, 'gamma': 0.7219988000669475, 'subsample': 0.9385588537448486}. Best is trial 0 with value: 0.7717304860821509.\u001b[0m\n","C:\\Users\\dmytro.hlushenkov\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[10:27:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2021-11-06 10:27:57,995]\u001b[0m Trial 3 finished with value: 0.7469841658363024 and parameters: {'max_depth': 3, 'n_estimators': 21, 'learning_rate': 0.6174815478795656, 'gamma': 0.6116531993229648, 'subsample': 0.007165598589195434}. Best is trial 0 with value: 0.7717304860821509.\u001b[0m\n","C:\\Users\\dmytro.hlushenkov\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[10:28:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2021-11-06 10:28:41,476]\u001b[0m Trial 4 finished with value: 0.7698712994139658 and parameters: {'max_depth': 2, 'n_estimators': 49, 'learning_rate': 0.524774707780923, 'gamma': 0.3998610317291583, 'subsample': 0.04676099664729407}. Best is trial 0 with value: 0.7717304860821509.\u001b[0m\n","C:\\Users\\dmytro.hlushenkov\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[10:28:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2021-11-06 10:29:17,829]\u001b[0m Trial 5 finished with value: 0.7701756931605228 and parameters: {'max_depth': 5, 'n_estimators': 15, 'learning_rate': 0.45607003861003753, 'gamma': 0.7851759828754175, 'subsample': 0.1997538147801439}. Best is trial 0 with value: 0.7717304860821509.\u001b[0m\n","C:\\Users\\dmytro.hlushenkov\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[10:29:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2021-11-06 10:32:36,558]\u001b[0m Trial 6 finished with value: 0.7710986213187567 and parameters: {'max_depth': 5, 'n_estimators': 64, 'learning_rate': 0.46676294657169065, 'gamma': 0.8599404207422798, 'subsample': 0.6803395078339209}. Best is trial 0 with value: 0.7717304860821509.\u001b[0m\n","C:\\Users\\dmytro.hlushenkov\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[10:32:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2021-11-06 10:33:02,403]\u001b[0m Trial 7 finished with value: 0.7712686447952423 and parameters: {'max_depth': 2, 'n_estimators': 18, 'learning_rate': 0.9488855423647795, 'gamma': 0.9656320365113561, 'subsample': 0.8084165083816495}. Best is trial 0 with value: 0.7717304860821509.\u001b[0m\n","C:\\Users\\dmytro.hlushenkov\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[10:33:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2021-11-06 10:34:50,846]\u001b[0m Trial 8 finished with value: 0.7712749649942096 and parameters: {'max_depth': 2, 'n_estimators': 90, 'learning_rate': 0.09767220423917247, 'gamma': 0.6842330580888543, 'subsample': 0.4402084784902273}. Best is trial 0 with value: 0.7717304860821509.\u001b[0m\n","C:\\Users\\dmytro.hlushenkov\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[10:34:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2021-11-06 10:35:15,210]\u001b[0m Trial 9 finished with value: 0.7712537713925683 and parameters: {'max_depth': 5, 'n_estimators': 8, 'learning_rate': 0.034388617676366286, 'gamma': 0.9093204111467419, 'subsample': 0.2588541036018569}. Best is trial 0 with value: 0.7717304860821509.\u001b[0m\n","C:\\Users\\dmytro.hlushenkov\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[10:35:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2021-11-06 10:42:05,426]\u001b[0m Trial 10 finished with value: 0.7709185150466654 and parameters: {'max_depth': 6, 'n_estimators': 115, 'learning_rate': 0.23581743523142756, 'gamma': 0.01796197381863207, 'subsample': 0.5595159899747266}. Best is trial 0 with value: 0.7717304860821509.\u001b[0m\n","C:\\Users\\dmytro.hlushenkov\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[10:42:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2021-11-06 10:48:27,237]\u001b[0m Trial 11 finished with value: 0.7720915317256372 and parameters: {'max_depth': 4, 'n_estimators': 146, 'learning_rate': 0.22393217989069103, 'gamma': 0.552650010393103, 'subsample': 0.9998603421226725}. Best is trial 11 with value: 0.7720915317256372.\u001b[0m\n","C:\\Users\\dmytro.hlushenkov\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[10:48:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2021-11-06 10:54:59,437]\u001b[0m Trial 12 finished with value: 0.7722962288398156 and parameters: {'max_depth': 4, 'n_estimators': 150, 'learning_rate': 0.23829480505076261, 'gamma': 0.5184304014395756, 'subsample': 0.9984415919487171}. Best is trial 12 with value: 0.7722962288398156.\u001b[0m\n","C:\\Users\\dmytro.hlushenkov\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[10:55:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2021-11-06 11:01:27,965]\u001b[0m Trial 13 finished with value: 0.771775569292644 and parameters: {'max_depth': 4, 'n_estimators': 141, 'learning_rate': 0.2974661654293682, 'gamma': 0.20684985219959368, 'subsample': 0.9941029199791809}. Best is trial 12 with value: 0.7722962288398156.\u001b[0m\n","C:\\Users\\dmytro.hlushenkov\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[11:01:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2021-11-06 11:06:12,917]\u001b[0m Trial 14 finished with value: 0.7718602511750011 and parameters: {'max_depth': 3, 'n_estimators': 129, 'learning_rate': 0.34197007075037944, 'gamma': 0.525253128348287, 'subsample': 0.8561473653787335}. Best is trial 12 with value: 0.7722962288398156.\u001b[0m\n","C:\\Users\\dmytro.hlushenkov\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[11:06:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","Optimized on ROC AUC score\n","Optimized XGBoost accuracy:  0.7718598630365793\n","Optimized XGBoost roc-auc-score:  0.7722962288398156\n"]}],"source":["def create_model(trial):\n","    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n","    n_estimators = trial.suggest_int(\"n_estimators\", 1, 150)\n","    learning_rate = trial.suggest_uniform('learning_rate', 0.0000001, 1)\n","    gamma = trial.suggest_uniform('gamma', 0.0000001, 1)\n","    subsample = trial.suggest_uniform('subsample', 0.0001, 1.0)\n","    model = XGBClassifier(\n","        learning_rate=learning_rate, \n","        n_estimators=n_estimators, \n","        max_depth=max_depth, \n","        gamma=gamma, \n","        subsample=subsample,\n","        random_state=42\n","    )\n","    return model\n","\n","optimizer = Optimizer('f1')\n","xgb_f1_params = optimizer.optimize()\n","xgb_f1_params['random_state'] = 42\n","xgb_f1 = XGBClassifier(\n","    **xgb_f1_params\n",")\n","xgb_f1.fit(X, y)\n","preds = xgb_f1.predict(X_val)\n","\n","print('Optimized on ROC AUC score')\n","print('Optimized XGBoost accuracy: ', accuracy_score(y_val, preds))\n","print('Optimized XGBoost roc-auc-score: ', roc_auc_score(y_val, preds))"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2021-11-05T20:49:25.953351Z","iopub.status.busy":"2021-11-05T20:49:25.950460Z","iopub.status.idle":"2021-11-05T22:22:16.524861Z","shell.execute_reply":"2021-11-05T22:22:16.523795Z","shell.execute_reply.started":"2021-11-05T20:49:25.953261Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2021-11-06 11:16:48,863]\u001b[0m A new study created in memory with name: no-name-9c080d86-b90e-43d6-b425-0c1fcbfb55f3\u001b[0m\n","\u001b[32m[I 2021-11-06 11:28:12,941]\u001b[0m Trial 0 finished with value: 0.7712645649026889 and parameters: {'max_depth': 5, 'n_estimators': 94, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.7712645649026889.\u001b[0m\n","\u001b[32m[I 2021-11-06 11:29:59,856]\u001b[0m Trial 1 finished with value: 0.7712644862189688 and parameters: {'max_depth': 6, 'n_estimators': 22, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.7712645649026889.\u001b[0m\n","\u001b[32m[I 2021-11-06 11:33:17,238]\u001b[0m Trial 2 finished with value: 0.7712645649026889 and parameters: {'max_depth': 3, 'n_estimators': 76, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.7712645649026889.\u001b[0m\n","\u001b[32m[I 2021-11-06 11:41:40,971]\u001b[0m Trial 3 finished with value: 0.7712645649026889 and parameters: {'max_depth': 6, 'n_estimators': 101, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.7712645649026889.\u001b[0m\n","\u001b[32m[I 2021-11-06 11:44:50,459]\u001b[0m Trial 4 finished with value: 0.7712697256065892 and parameters: {'max_depth': 4, 'n_estimators': 54, 'min_samples_leaf': 2}. Best is trial 4 with value: 0.7712697256065892.\u001b[0m\n","\u001b[32m[I 2021-11-06 11:47:37,162]\u001b[0m Trial 5 finished with value: 0.7712645649026889 and parameters: {'max_depth': 5, 'n_estimators': 39, 'min_samples_leaf': 2}. Best is trial 4 with value: 0.7712697256065892.\u001b[0m\n","\u001b[32m[I 2021-11-06 11:49:11,583]\u001b[0m Trial 6 finished with value: 0.7712697649484492 and parameters: {'max_depth': 5, 'n_estimators': 22, 'min_samples_leaf': 1}. Best is trial 6 with value: 0.7712697649484492.\u001b[0m\n","\u001b[32m[I 2021-11-06 11:53:37,665]\u001b[0m Trial 7 finished with value: 0.7712645649026889 and parameters: {'max_depth': 5, 'n_estimators': 59, 'min_samples_leaf': 6}. Best is trial 6 with value: 0.7712697649484492.\u001b[0m\n","\u001b[32m[I 2021-11-06 12:01:29,866]\u001b[0m Trial 8 finished with value: 0.7712645649026889 and parameters: {'max_depth': 6, 'n_estimators': 90, 'min_samples_leaf': 1}. Best is trial 6 with value: 0.7712697649484492.\u001b[0m\n","\u001b[32m[I 2021-11-06 12:04:56,720]\u001b[0m Trial 9 finished with value: 0.7712593255150685 and parameters: {'max_depth': 4, 'n_estimators': 60, 'min_samples_leaf': 10}. Best is trial 6 with value: 0.7712697649484492.\u001b[0m\n","\u001b[32m[I 2021-11-06 12:08:44,785]\u001b[0m Trial 10 finished with value: 0.7712645649026889 and parameters: {'max_depth': 2, 'n_estimators': 133, 'min_samples_leaf': 4}. Best is trial 6 with value: 0.7712697649484492.\u001b[0m\n","\u001b[32m[I 2021-11-06 12:09:44,662]\u001b[0m Trial 11 finished with value: 0.7712540467855882 and parameters: {'max_depth': 4, 'n_estimators': 18, 'min_samples_leaf': 3}. Best is trial 6 with value: 0.7712697649484492.\u001b[0m\n","\u001b[32m[I 2021-11-06 12:09:52,553]\u001b[0m Trial 12 finished with value: 0.7711482824069253 and parameters: {'max_depth': 3, 'n_estimators': 3, 'min_samples_leaf': 1}. Best is trial 6 with value: 0.7712697649484492.\u001b[0m\n","\u001b[32m[I 2021-11-06 12:11:26,856]\u001b[0m Trial 13 finished with value: 0.7712852470601507 and parameters: {'max_depth': 3, 'n_estimators': 39, 'min_samples_leaf': 4}. Best is trial 13 with value: 0.7712852470601507.\u001b[0m\n","\u001b[32m[I 2021-11-06 12:11:33,950]\u001b[0m Trial 14 finished with value: 0.7712645649026889 and parameters: {'max_depth': 2, 'n_estimators': 4, 'min_samples_leaf': 4}. Best is trial 13 with value: 0.7712852470601507.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Optimized on ROC AUC score\n","Optimized Random Forest:  0.7708107148822448\n","Optimized Random Forest roc-auc-score:  0.7712852470601507\n","Wall time: 56min 20s\n"]}],"source":["%%time\n","def create_model(trial):\n","    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n","    n_estimators = trial.suggest_int(\"n_estimators\", 2, 150)\n","    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n","    model = RandomForestClassifier(\n","        min_samples_leaf=min_samples_leaf, \n","        n_estimators=n_estimators, \n","        max_depth=max_depth, \n","        random_state=42\n","    )\n","    return model\n","\n","optimizer = Optimizer('roc_auc')\n","rf_f1_params = optimizer.optimize()\n","rf_f1_params['random_state'] = 42\n","rf_f1 = RandomForestClassifier(\n","    **rf_f1_params\n",")\n","rf_f1.fit(X, y)\n","preds = rf_f1.predict(X_val)\n","\n","print('Optimized on ROC AUC score')\n","print('Optimized Random Forest: ', accuracy_score(y_val, preds))\n","print('Optimized Random Forest roc-auc-score: ', roc_auc_score(y_val, preds))\n","# ~ 1 hour for 15 rounds of hyperparameter tuning "]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["from collections import Counter\n","from sklearn.tree import DecisionTreeClassifier\n","np.random.seed(42)\n","def bootstrap_sample(X, y):\n","    n_samples = X.shape[0]\n","    idxs = np.random.choice(n_samples, n_samples, replace=True)\n","    return X[idxs], y[idxs]\n","\n","\n","def most_common_label(y):\n","    counter = Counter(y)\n","    most_common = counter.most_common(1)[0][0]\n","    return most_common\n","class RandomForest:\n","    def __init__(self, n_trees=100):\n","        self.n_trees = n_trees\n","\n","        self.trees = []\n","\n","    def fit(self, X, y):\n","        self.trees = []\n","        for _ in range(self.n_trees):\n","            tree = DecisionTreeClassifier(\n","                criterion='gini', max_depth=None, min_samples_split=2, \n","                min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, \n","                min_impurity_decrease=0.0, min_impurity_split=None,  \n","            )\n","            X_samp, y_samp = bootstrap_sample(X, y)\n","            tree.fit(X_samp, y_samp)\n","            self.trees.append(tree)\n","\n","    def predict(self, X):\n","        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n","        tree_preds = np.swapaxes(tree_preds, 0, 1)\n","        y_pred = [most_common_label(tree_pred) for tree_pred in tree_preds]\n","        return np.array(y_pred)\n","\n","my_rf_cls = RandomForest()\n","my_rf_cls.fit(X.values, y.values)\n","# ~ takes 60 minutes with fixed hyperparameters"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Custom Random Forest ROC AUC SCORE:  0.7707578276333077\n"]}],"source":["preds = my_rf_cls.predict(X_val)\n","print('Custom Random Forest ROC AUC SCORE: ', roc_auc_score(y_val, preds))"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2021-11-05T22:35:47.356307Z","iopub.status.busy":"2021-11-05T22:35:47.355933Z","iopub.status.idle":"2021-11-05T23:23:13.675973Z","shell.execute_reply":"2021-11-05T23:23:13.673098Z","shell.execute_reply.started":"2021-11-05T22:35:47.356271Z"},"trusted":true},"outputs":[],"source":["%%time\n","models = [\n","    ('lgbm', LGBMClassifier(**lgb_f1_params)),\n","    ('rf', RandomForestClassifier(**rf_f1_params)),\n","    ('xgboost', XGBClassifier(**xgb_f1_params))\n","]\n","\n","kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n","oof_pred_tmp = dict()\n","test_pred_tmp = dict()\n","scores_tmp = dict()\n","\n","for fold, (idx_train, idx_valid) in enumerate(kf.split(X, y)):\n","    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n","    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n","    \n","    for name, model in models:\n","        if name not in scores_tmp:\n","            oof_pred_tmp[name] = list()\n","            oof_pred_tmp['y_valid'] = list()\n","            test_pred_tmp[name] = list()\n","            scores_tmp[name] = list()\n","     \n","        if name != 'rf':\n","            model.fit(\n","                X_train, y_train,\n","                eval_set=[(X_valid,y_valid)],\n","                verbose=0\n","            )\n","        else:\n","            model.fit(\n","                X_train, y_train,\n","            )\n","        \n","        pred_valid = model.predict_proba(X_valid)[:,1]\n","        score = roc_auc_score(y_valid, pred_valid)\n","        \n","        scores_tmp[name].append(score)\n","        oof_pred_tmp[name].extend(pred_valid)\n","        \n","        print(f\"Fold: {fold + 1} Model: {name} Score: {score}\")\n","        print('--'*20)\n","        \n","        y_hat = model.predict_proba(test)[:,1]\n","        test_pred_tmp[name].append(y_hat)\n","    \n","    oof_pred_tmp['y_valid'].extend(y_valid)\n","        \n","for name, model in models:\n","    print(f\"Overall Validation Score | {name}: {np.mean(scores_tmp[name])}\")\n","    print('='*20)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2021-11-05T23:23:13.684402Z","iopub.status.busy":"2021-11-05T23:23:13.683917Z","iopub.status.idle":"2021-11-05T23:23:19.068552Z","shell.execute_reply":"2021-11-05T23:23:19.067570Z","shell.execute_reply.started":"2021-11-05T23:23:13.684312Z"},"trusted":true},"outputs":[],"source":["base_test_predictions = pd.DataFrame(\n","    {name: np.mean(np.column_stack(test_pred_tmp[name]), axis=1) \n","    for name in test_pred_tmp.keys()}\n",")\n","\n","base_test_predictions['simple_avg'] = base_test_predictions.mean(axis=1)\n","simple_blend_submission = sample_submission.copy()\n","simple_blend_submission['claim'] = base_test_predictions['simple_avg']\n","simple_blend_submission.to_csv('./simple_blend_submission.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["![alt text](subm.png \"Title\")"]}],"metadata":{"interpreter":{"hash":"0f3ad2d559e150037d3aea5fed8327b1752ca5eee44973edca35cda80d58ba91"},"kernelspec":{"display_name":"Python 3.8.5 64-bit ('base': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":4}
